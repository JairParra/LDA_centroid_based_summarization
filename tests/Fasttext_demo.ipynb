{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastext_demo : Alignment of Multilingual Word Embeddings \n",
    "\n",
    "The following demo is a script based on Alex Meritts Usage at https://github.com/JairParra/NLPChallengeAI4Good/blob/master/TextClassification/Text%20classification.ipynb for the AI4Good NLPChallenge. \n",
    "\n",
    "The idea is the following: we are given a huge English and French dataset with text in both English and French, and the idea is to clasisfy the text according to a given set of labels; however, the text itself is unlabeled. \n",
    "\n",
    "We will then make usage of multilingual word vector alignments so that we can identify the text in any language (say English **or** French) , and then assign an appropriate set of labels that are the most correlated. These labels could be in any language! \n",
    "\n",
    "For this, we need the appropriate pre-trained word vectors. We will be borrowing the fastText_multilingual fastText module/wrapper (1) as well as the given language word vectors from Facebook fasttext. We will also need the `langdetect` module, as well as `spacy` , which will help us better pre-process the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requires the following dependencies: \n",
    "1. https://github.com/Babylonpartners/fastText_multilingual (wrapper + fastText vector alignment) \n",
    "2. https://fasttext.cc/docs/en/crawl-vectors.html (to download the vectors) \n",
    "3. https://spacy.io/ (spaCy)  \n",
    "4. https://pypi.org/project/langdetect/ (obvious...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # for language pre-processing \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from fasttext import FastVector \n",
    "from langdetect import detect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the tools \n",
    "\n",
    "We will obtain do the following:  \n",
    "- Obtain stopwords for both languages\n",
    "- Initialize both language models with spaCy \n",
    "- load the vectors into appropriate objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain stopwords for both English and French  \n",
    "english_stopwords = stopwords.words('french') \n",
    "french_stopwords = stopwords.words('english') \n",
    "\n",
    "# initialize spaCy's medium news language model and english model \n",
    "nlp_fr = spacy.load(\"fr_core_news_md\") \n",
    "nlp_en = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the vectors Note that here I am loading the vectors directed from a hardcoded PATH. In practice **you shouldn't do this**, but instead use a relative PATH. The reason why I'm doing this here is because the vectors themselves are quite heavy (~4G big or more), and since I'm using them for various projects in different locations, it doesn't make sense to have multiple copies everywhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_PATH = \"C:\\Users\\jairp\\Desktop\\BackUP\\CODE-20180719T021021Z-001\\CODE\\Python\\Datasets\\vectors\"\n",
    "fr_vecs_PATH = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
