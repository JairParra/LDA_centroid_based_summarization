{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastext_demo : Alignment of Multilingual Word Embeddings \n",
    "\n",
    "The following demo is a script based on Alex Meritts Usage at https://github.com/JairParra/NLPChallengeAI4Good/blob/master/TextClassification/Text%20classification.ipynb for the AI4Good NLPChallenge. \n",
    "\n",
    "The idea is the following: we are given a huge English and French dataset with text in both English and French, and the idea is to clasisfy the text according to a given set of labels; however, the text itself is unlabeled. \n",
    "\n",
    "We will then make usage of multilingual word vector alignments so that we can identify the text in any language (say English **or** French) , and then assign an appropriate set of labels that are the most correlated. These labels could be in any language! \n",
    "\n",
    "For this, we need the appropriate pre-trained word vectors. We will be borrowing the fastText_multilingual fastText module/wrapper (1) as well as the given language word vectors from Facebook fasttext. We will also need the `langdetect` module, as well as `spacy` , which will help us better pre-process the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requires the following dependencies: \n",
    "1. https://github.com/Babylonpartners/fastText_multilingual (wrapper + fastText vector alignment) \n",
    "2. https://fasttext.cc/docs/en/crawl-vectors.html (to download the vectors) \n",
    "3. https://spacy.io/ (spaCy)  \n",
    "4. https://pypi.org/project/langdetect/ (obvious...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import spacy # for language pre-processing \n",
    "import gensim \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile \n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec \n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "from fasttext import FastVector \n",
    "from langdetect import detect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the tools \n",
    "\n",
    "We will obtain do the following:  \n",
    "- Obtain stopwords for both languages\n",
    "- Initialize both language models with spaCy \n",
    "- load the vectors into appropriate objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain stopwords for both English and French  \n",
    "english_stopwords = stopwords.words('french') \n",
    "french_stopwords = stopwords.words('english') \n",
    "\n",
    "# initialize spaCy's medium news language model and english model \n",
    "nlp_fr = spacy.load(\"fr_core_news_md\") \n",
    "nlp_en = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the vectors Note that here I am loading the vectors directed from a hardcoded PATH. In practice **you shouldn't do this**, but instead use a relative PATH. The reason why I'm doing this here is because the vectors themselves are quite heavy (~4G big or more), and since I'm using them for various projects in different locations, it doesn't make sense to have multiple copies everywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading the fasttext Facebook research vectors \n",
    "\n",
    "First we obtain the path the vectos, then we load them using the FastVector class from the `fasttext` module. Notice that these embedding files are HUGE, so loading each will take roughly between 2 and 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jairp\\Desktop\\BackUP\\CODE-20180719T021021Z-001\\CODE\\Python\\Datasets\\vectors\\cc.fr.300.vec\n"
     ]
    }
   ],
   "source": [
    "# direct path to source folder , then join the paths to each vectors file\n",
    "direct_PATH = \"C:\\\\Users\\\\jairp\\\\Desktop\\\\BackUP\\\\CODE-20180719T021021Z-001\\\\CODE\\\\Python\\\\Datasets\\\\vectors\"\n",
    "fr_vecs_PATH = os.path.join(direct_PATH, \"cc.fr.300.vec\") \n",
    "en_vecs_PATH = os.path.join(direct_PATH, \"cc.en.300.vec\") \n",
    "\n",
    "print(fr_vecs_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from C:\\Users\\jairp\\Desktop\\BackUP\\CODE-20180719T021021Z-001\\CODE\\Python\\Datasets\\vectors\\cc.fr.300.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading vector...: 2000000it [03:39, 9125.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# load the French vectors\n",
    "t0 = time.time()\n",
    "fr_dictionary = FastVector(vector_file=fr_vecs_PATH, encoding='utf-8')\n",
    "t1 = time.time() \n",
    "print(\"Done in {} seconds.\".format(t1-t0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.193409442901611\n"
     ]
    }
   ],
   "source": [
    "# laod the alignment matrix \n",
    "t0 = time.time()\n",
    "fr_dictionary.apply_transform('alignment_matrices/fr.txt')\n",
    "t1 = time.time() \n",
    "print(\"Done in {} seconds.\".format(t1-t0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from C:\\Users\\jairp\\Desktop\\BackUP\\CODE-20180719T021021Z-001\\CODE\\Python\\Datasets\\vectors\\cc.en.300.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading vector...: 2000000it [03:48, 8751.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 228.54795336723328 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load the English vectors \n",
    "t0 = time.time()\n",
    "en_dictionary = FastVector(vector_file=en_vecs_PATH, encoding='utf-8')\n",
    "t1 = time.time() \n",
    "print(\"Done in {} seconds.\".format(t1-t0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 46.83586049079895 seconds.\n"
     ]
    }
   ],
   "source": [
    "# laod the alignment matrix \n",
    "t0 = time.time()\n",
    "fr_dictionary.apply_transform('alignment_matrices/fr.txt')\n",
    "t1 = time.time() \n",
    "print(\"Done in {} seconds.\".format(t1-t0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the data \n",
    "\n",
    "First, we will only load the labels from the excel file in Sheet1. We read in binary model so it is faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autonomie', \"Bris de l'isolement\", 'Communication', 'Compétences', 'Confiance en soi', 'Connaissance de soi', 'Connaissances', 'Conscientisation / Esprit critique', \"Développement (de l'enfant)\", 'Développement de pratiques démocratiques', 'Empowerment collectif', 'Empowerment individuel', 'Estime de soi', 'Habiletés cognitives', 'Habiletés dans la vie quotidienne', 'Habiletés sociales', 'Habitudes de vie', 'Identification des besoins', 'Intégration sociale', 'Lien de confiance', 'Liens familiaux', 'Mixité sociale et culturelle', 'Participation citoyenne', 'Plaisir', 'Prise de parole', 'Réciprocité', 'Répit', \"Réseau d'entraide\", 'Résultats scolaires', 'Sécurité', \"Sentiment d'appartenance\", 'Sentiment de valorisation', 'Socialisation', 'Soutien']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(open('Baseresultats.xlsx', 'rb'),\n",
    "              sheet_name='Sheet1', header=None)\n",
    "indicators = df[0].tolist()\n",
    "indicators = [x.strip() for x in indicators]\n",
    "\n",
    "print(indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining useful functions \n",
    "\n",
    "We will define a couple of functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity  \n",
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / n1 / n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(word, lang):\n",
    "    \"\"\" \n",
    "    Returns the vector representation of the input word \n",
    "    according to the language specified. If an Exception \n",
    "    is raised, returns a vector of zeros. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if(lang == 'en'):\n",
    "            return en_dictionary[word]\n",
    "        elif(lang == 'fr'):\n",
    "            return fr_dictionary[word]\n",
    "        else:\n",
    "            print(\"Found a non-English, non-French doc. Language detected : \" + lang)\n",
    "    except:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docIndicatorSimilarity(indicator, row, doc_lang): \n",
    "    \"\"\"\n",
    "    args: \n",
    "        @ indicator: \n",
    "        @ row: \n",
    "        @ doc: input document  \n",
    "        @ doc_lang: input document language \n",
    "    \"\"\"\n",
    "    \n",
    "    # declare tags to filter in the text \n",
    "    tags = [\"DET\",\"ADP\",\"PUNCT\",\"CONJ\",\"CCONJ\",\"NUM\",\"SYM\",\"SPACE\"]\n",
    "    \n",
    "    # tokenize by spaces\n",
    "    indicator_words = indicator.split(' ')\n",
    "    \n",
    "    # obtain word vectors \n",
    "    indicator_word_vecs = [vectorByLanguage(word, 'fr') for word in indicator_words]\n",
    "    \n",
    "    # vector of means of indicators by column \n",
    "    indicator_avg_vec = np.mean(indicator_word_vecs, axis=0)\n",
    "    \n",
    "    doc = str(row['RESULTATS_2018']) \n",
    "    \n",
    "    # obtain sentences \n",
    "    sent_toks = [sent for sent in nlp_fr(doc).sents]\n",
    "    \n",
    "    # clean the setnecne tokens by filtering certain tags and stopwords, numbers, etc. \n",
    "    flt_sent_toks = [[token for token in toks if token.pos_ not in tags and token.text.isalpha()\n",
    "                     and token.text not in french_stopwords] for toks in sent_toks]\n",
    "    \n",
    "    \n",
    "    flat_flt_sent_toks = [item.text for sublist in flt_sent_toks for item in sublist]\n",
    "    doc_words = doc.split(' ')\n",
    "\n",
    "    doc_word_vecs = [vectorByLanguage(word, doc_lang) for word in flat_flt_sent_toks]\n",
    "    doc_avg_vec = np.mean(doc_word_vecs, axis=0)\n",
    "    return similarity(indicator_avg_vec, doc_avg_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the word vectors with gensim instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5fcb3020a048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the french vectors. This might take a good damn while\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_fr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr_vecs_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the french vectors. This might take a good damn while \n",
    "model_fr = KeyedVectors.load_word2vec_format(fr_vecs_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English vectors\n",
    "model_en = KeyedVectors.load_word2vec_format(en_vecs_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model_fr['aimer'] \n",
    "\n",
    "# We can see the shape of the vector \n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      ", 0.0058 0.0478 0.1094 -0.0839 -0.2092 0.0072 -0.0780 0.0683 0.0120 -0.0314 -0.0695 -0.0938 -0.0006 0.0257 0.0215 0.1130 0.0517 0.0191 -0.0224 -0.0168 0.0723 0.0711 -0.0505 -0.0987 -0.0960 -0.0695 0.0191 -0.0003 -0.1440 -0.0528 0.0305 0.0586 -0.0246 0.0195 -0.0040 0.0421 -0.0361 0.0546 0.1568 0.0482 -0.0072 -0.0352 -0.0004 0.1192 0.1274 0.1168 -0.0188 -0.0482 0.0467 0.0487 -0.0213 -0.0177 -0.0399 0.0466 0.0376 -0.0011 0.0841 0.0149 -0.2848 0.0367 0.0917 0.0908 0.0493 -0.1145 0.0352 -0.0179 -0.0245 0.0516 0.0297 0.0141 -0.0582 -0.0562 -0.1111 -0.0624 -0.1561 -0.0105 0.0271 -0.0011 0.0857 0.0516 -0.0387 -0.0856 0.0198 -1.1291 -0.0349 -0.0315 0.0705 -0.0057 -0.0195 -0.0522 0.0336 -0.0265 0.0823 0.0362 0.0892 -0.0831 -0.0747 0.1039 -0.0266 0.4814 0.0162 -0.0484 -0.0033 0.0761 -0.0312 0.0213 -0.0188 0.0121 -0.0537 0.0473 0.0583 0.0292 0.0655 0.0111 0.1129 0.0659 -0.0759 0.0795 -0.0425 -0.0335 -0.1145 0.0100 0.0197 -0.1981 -0.0385 0.0319 0.0612 0.0273 -0.0384 0.0350 -0.0085 -0.2808 -0.0483 0.1094 0.0141 0.0280 -0.3553 -0.0018 0.0878 0.0226 0.0074 0.0249 0.0157 -0.0593 0.1134 0.0043 -0.0030 0.0075 0.0047 0.0009 -0.0156 0.0411 0.0221 -0.0635 0.0872 -0.0235 0.0572 0.0800 0.0981 0.0092 0.0833 -0.0935 0.0160 0.0331 -0.0713 0.0418 -0.0328 0.2146 0.1109 -0.0183 -0.1887 -0.0295 0.0015 -0.0841 -0.0299 0.1776 -0.0621 -0.1737 -0.0194 -0.0427 0.0805 0.0132 0.2168 -0.0151 -0.0302 0.0126 -0.1072 0.1783 -0.2433 -0.0150 0.0366 -0.1321 -0.1489 -0.0507 -0.0045 0.0401 0.0188 -0.1100 -0.0022 -0.2031 0.0472 0.0948 0.0517 0.0343 -0.0175 -0.0844 -0.1151 -0.1123 -0.0403 0.1087 -0.0712 -0.0138 0.6086 0.0123 -0.0308 -0.0048 0.0968 -0.0526 0.0608 -0.0281 0.0368 -0.0485 0.0043 0.0713 -0.0074 -0.1229 -0.0109 0.0587 0.0459 -0.0537 -0.0332 -0.0436 0.1253 0.0157 -0.0313 0.0769 0.0397 0.1136 -0.1049 0.2397 0.0215 0.0275 -0.0489 -0.0259 0.0276 -0.0055 0.0183 0.0020 -0.0048 -0.0506 -0.1030 0.0259 -0.0489 -0.0113 0.0057 0.0449 0.0179 -0.0285 -0.0358 0.0000 0.1036 -0.0939 -0.0243 -0.0405 0.0220 -0.0312 0.0279 0.0325 0.0260 -0.0166 0.0271 -0.0841 -0.1063 -0.0577 -0.0141 -0.0337 0.1020 -0.1223 -0.0383 -0.0064 -0.0906 -0.0013 -0.0714 0.0688 0.0502 0.0701 -0.0467 0.0641 0.1946 0.0127 0.0667 -0.0513 0.0051 -0.0071 0.0439 -0.1100 0.0359 -0.3931 0.0230 0.0378\n",
      "\n",
      "de -0.0842 -0.0388 0.0456 -0.0559 -0.0366 0.0241 0.0919 -0.0214 0.0179 -0.1384 -0.0202 -0.1276 -0.0163 0.0644 -0.1042 0.0152 -0.0191 0.0761 -0.0149 0.0261 0.0354 -0.0770 -0.0034 0.0941 -0.0169 0.1621 0.2469 -0.0090 0.0335 0.0022 -0.0168 -0.0063 0.0149 -0.0182 0.0205 0.0628 -0.3591 -0.0155 0.0188 0.0503 -0.0251 0.0328 0.0400 0.0639 -0.1502 0.1655 0.0538 0.0762 -0.1086 -0.0351 0.0534 0.0267 0.0255 0.0380 0.0026 0.3703 0.0797 -0.0189 0.4854 0.0882 0.0483 0.2240 0.0077 -0.2437 -0.0396 -0.0343 -0.1632 -0.0818 -0.0074 0.0008 -0.0255 -0.0482 -0.4431 -0.0576 -0.0413 -0.0182 -0.0852 -0.0737 0.2608 -0.0044 -0.0147 -0.0486 -0.2496 -1.3323 -0.0243 -0.0382 0.0852 0.0166 0.0292 -0.0092 0.0345 -0.0205 0.0806 -0.0287 0.0068 -0.3224 -0.0187 -0.0661 -0.0430 0.4115 0.0210 0.0019 0.0826 0.0753 0.0254 0.0634 0.0524 -0.0342 -0.0224 0.3635 0.0102 -0.0121 -0.3234 0.1405 0.0347 0.0290 -0.0187 0.0473 -0.0670 0.0084 -0.0503 -0.0469 -0.1019 0.1343 -0.0289 0.0632 0.0699 0.0675 0.0196 -0.0432 0.0576 0.0173 0.0264 0.0001 0.0260 -0.0262 -0.3346 -0.0250 0.1202 0.0655 0.0264 -0.0396 0.0032 -0.0192 -0.0364 -0.0285 0.0278 0.0017 -0.0048 -0.0001 -0.0395 0.0020 -0.1174 0.0715 0.0118 -0.0433 0.0497 -0.0519 0.0654 -0.0596 0.0060 0.1493 0.0100 0.0117 -0.1024 -0.0334 0.0252 -0.2275 -0.0043 -0.0623 0.3386 0.0622 0.0344 -0.3352 -0.0398 -0.1610 -0.0401 -0.2124 0.0329 0.0056 -0.0218 -0.0070 0.1279 0.0429 -0.0155 0.0529 0.1669 0.0851 -0.4496 -0.0199 0.1243 0.0296 0.0625 0.5931 -0.0495 -0.0263 0.0038 0.0456 -0.0591 0.0706 0.0460 0.0196 0.0271 0.0136 0.0427 0.1151 0.0651 0.0513 0.3261 -0.0095 -0.1681 0.0631 0.4491 0.0119 -0.0168 -0.0606 -0.2383 -0.0494 0.1051 0.0095 -0.0175 -0.0459 0.0940 0.0788 0.0581 -0.0833 0.0291 0.0228 0.0040 -0.2135 -0.0450 -0.2637 -0.0708 -0.0272 0.0321 -0.0116 0.0079 -0.0634 0.1234 -0.0904 0.0501 -0.0339 -0.0494 0.0714 0.1486 0.1024 0.0903 0.0458 -0.0289 -0.0185 -0.0340 0.0427 -0.0330 -0.0147 -0.2744 -0.0971 0.0208 0.0127 -0.0412 0.0009 -0.0658 0.0333 -0.0383 0.0523 -0.0190 0.0391 0.0702 0.0231 0.0573 0.0830 -0.1997 -0.0273 -0.0001 0.0020 -0.0557 0.0669 -0.0026 0.1349 0.0173 -0.0312 -0.0388 0.0320 0.0129 -0.0233 0.0034 -0.0373 0.0239 -0.0700 0.0412 0.0402 0.0019 -0.0405 -0.0111 -0.0038 0.0080 0.1887 0.0118 0.3069 -0.0106 0.0579\n",
      "\n",
      ". -0.0440 0.0455 0.0270 -0.0400 -0.0903 0.0190 -0.1007 -0.0530 0.0496 -0.1073 0.0145 0.2207 0.0471 0.0280 -0.0939 0.0162 0.0700 0.0134 -0.0966 0.0274 0.1434 0.0315 -0.0831 0.1496 -0.0355 0.0732 0.1246 0.0323 0.0351 -0.0326 -0.0931 0.0590 -0.1116 -0.0008 0.0914 0.0403 -0.3949 0.0609 0.0217 -0.0603 -0.0322 0.0393 0.0874 0.0341 -0.1597 0.2018 -0.0101 0.0502 -0.0494 0.0551 0.0082 -0.0388 0.0449 0.1211 -0.1171 -0.4991 -0.0103 0.1310 -0.0801 0.1505 0.0254 -0.1259 -0.0205 0.1411 0.0038 -0.0641 -0.1475 -0.0361 0.1126 0.0334 -0.0239 0.1092 -0.1026 -0.0552 -0.2524 -0.0731 -0.0509 -0.0039 0.4030 0.0251 -0.0449 -0.0085 0.1035 -1.0269 -0.0279 -0.0645 0.0661 0.1138 0.0435 0.0522 0.0758 -0.0447 0.0313 -0.0793 0.0800 0.0480 -0.0039 0.0358 -0.1036 0.8045 0.0015 0.0344 -0.0167 -0.0055 0.0809 0.0604 -0.0015 -0.0524 -0.0576 -0.2323 0.0015 0.0189 0.0909 -0.0734 0.0985 -0.0134 0.0710 0.0698 -0.0820 0.0012 -0.1980 0.0901 -0.0614 0.2198 -0.0505 0.0655 -0.0259 0.1051 -0.0668 -0.0241 0.1170 -0.3131 -0.0215 -0.0371 0.0463 -0.1121 -0.4985 -0.0551 0.0110 0.0420 -0.0097 0.0808 0.0592 0.0387 0.0208 0.0153 0.0089 -0.0255 0.0333 -0.0216 -0.2149 -0.0411 -0.0768 -0.2455 -0.1296 -0.0266 0.0747 -0.2930 0.0138 -0.0653 -0.0553 0.0232 0.0039 -0.1654 0.1165 -0.0505 -0.0230 0.0953 -0.0167 -0.0356 -0.0760 0.0891 0.0011 0.2175 -0.1144 -0.0714 -0.0288 0.0899 -0.0761 -0.1090 0.0518 0.0316 -0.2928 0.0275 0.0723 -0.0238 -0.1000 0.1858 0.1424 -0.0326 0.0417 0.0735 -0.1133 -0.5320 0.0419 0.0357 -0.1436 -0.0983 0.0713 -0.0719 -0.0255 0.4678 0.0037 -0.0319 -0.0024 0.0339 -0.1713 -0.1064 0.4095 -0.0054 0.0785 -0.1027 0.5203 0.0621 -0.0154 -0.2844 0.1029 -0.1153 -0.0058 0.1245 -0.0148 -0.0382 0.0969 0.0246 0.0246 0.0579 -0.0393 0.0190 0.0729 0.3230 -0.1100 -0.0321 0.1313 0.0360 -0.0732 0.0899 0.0339 0.0388 -0.0575 0.0194 0.1430 0.0626 -0.0041 0.0022 -0.1552 -0.0291 -0.1234 -0.0218 -0.0095 -0.1327 -0.0758 0.0475 -0.1518 -0.0429 -0.0716 0.0511 0.0325 0.2686 -0.0607 0.0554 -0.1901 0.0299 -0.0789 -0.0038 -0.0780 0.0670 0.1079 -0.0031 0.0519 0.0535 -0.2188 -0.0012 -0.0743 -0.0339 -0.0678 0.0226 -0.0126 0.0106 -0.0443 0.1022 -0.0337 0.0136 0.0515 0.0320 0.0376 -0.0392 -0.0208 0.0022 0.0668 -0.0293 0.0895 -0.0071 0.0003 -0.0094 0.0582 -0.3653 0.0605 -0.5981 0.0708 0.0658\n",
      "\n",
      "</s> 0.0112 -0.0152 0.0474 -0.0603 0.0614 0.0330 0.1578 0.0725 -0.0501 -0.2283 -0.0614 0.4037 -0.0787 -0.0288 -0.1229 0.0040 -0.0488 0.0341 -0.0958 0.0836 0.0281 0.0436 -0.0431 0.0373 -0.0692 0.1221 0.5740 -0.0140 0.2898 0.0240 -0.0705 -0.1272 0.0151 0.0373 0.0169 0.0642 -0.7611 -0.0044 -0.0119 -0.0206 0.1266 0.0182 0.0049 0.1165 0.0850 0.4208 -0.0518 -0.0344 -0.0480 0.0045 -0.0256 -0.0396 0.0008 0.1827 -0.0569 0.0010 -0.1249 0.1034 0.0143 -0.0251 0.0689 -0.3740 0.1163 0.1787 -0.0342 -0.0655 -0.2273 -0.0641 0.0046 0.0664 0.1383 -0.1607 -0.0460 -0.0056 0.0030 0.1407 0.0411 -0.0956 0.2750 0.0466 -0.0532 0.0686 0.0404 -1.4073 0.1001 0.0459 0.0150 0.0144 0.0800 0.0483 0.1400 0.0224 -0.0761 -0.0591 -0.1318 -0.3028 0.0005 0.1514 0.0742 0.8294 0.0473 0.0583 0.0013 0.0716 0.0775 0.0627 -0.0797 -0.0579 -0.0375 -0.0994 -0.0648 -0.0375 -0.0405 0.0031 0.0335 0.0412 0.0179 0.0895 0.0262 0.0243 0.1223 0.0329 -0.0247 0.0049 -0.0154 0.0664 0.0035 -0.0029 0.0036 0.0022 0.0539 -0.4254 -0.0228 0.0929 0.0142 -0.1148 -0.7674 -0.0085 -0.0468 0.0370 -0.0576 0.0062 0.0399 0.1282 -0.0082 -0.0390 0.0129 -0.0329 -0.0081 -0.0269 -1.3358 -0.0737 -0.0841 -0.3369 -0.0051 0.0796 0.0991 -0.1354 0.0115 0.0021 0.1097 0.0182 0.0474 0.0689 0.2108 -0.0760 -0.0012 0.2437 0.0364 -0.1338 -0.1164 0.1124 0.0085 0.3678 -0.0601 0.0468 -0.0322 0.1583 0.0084 -0.1932 -0.0057 -0.0314 -0.4364 -0.0959 -0.1025 -0.0341 -0.0195 0.4907 -0.0711 0.0011 0.4158 0.1341 -0.3885 -0.3772 0.0077 -0.0210 -0.2724 -0.0017 -0.0305 -0.1196 0.0558 0.1919 0.0458 0.0759 0.0635 -0.1137 0.0577 -0.3529 0.1880 -0.0582 -0.1906 -0.2723 0.8613 0.0185 -0.0314 -0.2117 0.1412 -0.0781 0.0019 0.0439 -0.0435 0.0423 0.0303 0.0145 -0.0454 -0.0181 -0.0440 -0.1367 -0.0268 0.1168 0.0188 0.0541 0.0105 -0.0236 -0.0157 0.0483 0.0563 0.0255 -0.0544 0.2093 0.2461 0.0174 -0.0514 0.1077 -0.4262 -0.0974 -0.2110 0.0280 -0.0916 0.0171 0.0303 0.0762 -0.1136 0.0073 0.0697 -0.2049 0.0085 0.0007 0.0418 0.0433 -0.0392 0.0156 -0.1742 0.0053 -0.0695 0.0481 0.0116 0.0343 -0.0462 0.0405 -0.2151 -0.0311 -0.2367 -0.0330 0.0522 -0.0964 0.1348 0.0304 0.0628 -0.0001 0.2177 -0.0129 -0.0526 0.0018 0.0497 -0.0742 0.0675 0.0246 0.1483 0.0254 0.0133 -0.0242 -0.0084 -0.0221 0.0049 -0.1825 0.0966 -0.8128 -0.0720 -0.0517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 5973: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-69a1854ad11d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr_vecs_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m<\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 5973: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open(fr_vecs_PATH,'r') as infile: \n",
    "    count = 0\n",
    "    for line in infile: \n",
    "        if count <5:\n",
    "            print(line) \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
